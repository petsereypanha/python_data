{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; background-color: #ef7d22; text-align: center\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    PostgreSQL adapters for Python\n",
    "</h1>\n",
    "\n",
    "<br><br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Python developers who access PostgreSQL databases use the `psycopg2` driver/adapter.  It is well tested, reliable, and conforms well to the DB-API 2.0 standard.  The successor `psycopg3` adapter is under development as of late 2020 and will probably become the default in the future.  Most user-facing behaviors will remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are three other notable options for adaptors that you may want to consider for your particular use case.\n",
    "\n",
    "* `pg8000` is a pure-Python implementation that complies with DB-API 2.0, and has no external dependencies outside of the standard library.  Very little will differ from a user-level perspective from using `psycopg2`, but in contexts where institutional or technical constraints do not allow you to use C-combiled third-party libraries, `pg8000` is an option.\n",
    "\n",
    "* `aiopg` is an `asyncio`-friendly wrapper around `psycopg2` that allows you to use PostgreSQL in asynchronous programs.  General programming styles and benefits of async are discussed in another INE course.  In a sentence though, for high-performance I/O bound programs ann asyncronous approach can be *vastly* faster than a thread-based or single-threaded one.  `aiopg` is *mostly* similar to the DB-API, but because of the nature of asynchronous programming, some differences arise.\n",
    "\n",
    "* `asyncpg` is another `asyncio`-friendly driver, but one that both aims to be *very* fast and that as a consequence is much less concerned with DB-API compatibility.  `asyncpg` is also more complete in its PostgreSQL-specific support, for example in directly supporting a full range of rich PostgreSQL datatypes, such as arrays and composite types.  `psycopg2` is limited by using the text-based communication protocol rather than the binary I/O protocol that PostgreSQL provides as an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-relational mapping\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Not covered in this course is object-relational mapping libraries that convert SQL and tuple-level interfaces with PostgreSQL into Python method calls.  In particular, `SQLAlchemy` is very popular among many Python developers.  Personally, I find the extra layer between code and database a distraction rather than a benefit.\n",
    "\n",
    "In any event though, `SQLAlchemy` relies on the actual PostgreSQL adapters we discuss in this lesson.  The abstractions it provides are not PostgreSQL specific, but generic patterns for accessing RDBMS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Python\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "Let us use the `pg8000` adapter to load some data into our database.  Here we will need to use a different parameter style than we saw with `psycopg2` in the first lesson, but most of the API is the same.  The thread safety level is lower for `pg8000` as well; we will need a separate connection per-thread if we program in a multi-threaded way with this adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API level       | 2.0\n",
      "Parameter style | format\n",
      "Thread safety   | 1\n"
     ]
    }
   ],
   "source": [
    "import pg8000\n",
    "print(f\"API level       | {pg8000.apilevel}\")\n",
    "print(f\"Parameter style | {pg8000.paramstyle}\")\n",
    "print(f\"Thread safety   | {pg8000.threadsafety}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need credentials and host/port setting for any of these adaptors, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Login(user='ine_student', host='localhost', database='ine', port='5432', password='ine-password')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Login = namedtuple(\"Login\", \"user host database port password\")\n",
    "login = Login('ine_student', 'localhost', 'ine', '5432', 'ine-password')\n",
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_pg8000 = pg8000.connect(*login)\n",
    "cur_pg8k = conn_pg8000.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a connection using this (named)tuple of information.  The order is the same as the argument order of the `.connect()` function, for convenience.  We could use named arguments to the function as well, if we preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding on schemata\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "As some data to load into the database, let us take some information on United States zip codes published by the U.S. Census Bureau.  We have two source files available.  One tab separated file that gives explanations of column names, and another that gives information per zip code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data associated with this notebook can be found in the files associated with this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS\tUnited States Postal Service State Abbreviation\n",
      "GEOID\tGeographic Identifier - fully concatenated geographic code (State FIPS and district number)\n",
      "ALAND\tLand Area (square meters) - Created for statistical purposes only\n",
      "AWATER\tWater Area (square meters) - Created for statistical purposes only\n",
      "ALAND_SQMI\tLand Area (square miles) - Created for statistical purposes only\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../data/census-zipcodes-2018.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USPS\tALAND\tAWATER\tALAND_SQMI\tAWATER_SQMI\tINTPTLAT\tINTPTLONG\n",
      "00601\t166659749\t799292\t64.348\t0.309\t18.180555\t-66.749961\n",
      "00602\t79307535\t4428429\t30.621\t1.71\t18.361945\t-67.175597\n",
      "00603\t81887185\t181411\t31.617\t0.07\t18.455183\t-67.119887\n",
      "00606\t109579993\t12487\t42.309\t0.005\t18.158327\t-66.932928\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../data/census-zipcodes-2018.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before putting the data into tables, we should decide on good table layouts.  The field key is relatively straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pg8000.core.Cursor at 0x7f04a4361f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_pg8k.execute('DROP TABLE IF EXISTS census_zipcode_fields;')\n",
    "\n",
    "sql_census_fields = \"\"\"\n",
    "CREATE TABLE census_zipcode_fields (\n",
    "  key VARCHAR(15) PRIMARY KEY,  -- by implication, UNIQUE NOT NULL\n",
    "  description VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "cur_pg8k.execute(sql_census_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set describing fields is small, and can easily be read into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pg8000.core.Cursor at 0x7f04a4361f70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = [tuple(line.strip().split('\\t')) \n",
    "          for line in open('../data/census-zipcodes-2018.fields')]\n",
    "\n",
    "sql = \"\"\"\n",
    "INSERT INTO census_zipcode_fields (key, description)\n",
    "VALUES (%s, %s)\n",
    "\"\"\"\n",
    "cur_pg8k.executemany(sql, fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The types for the main data allows us to use the data types of PostgreSQL more versatilely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pg8000.core.Cursor at 0x7f04a4361f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_pg8k.execute('DROP TABLE IF EXISTS census_zipcode_geography;')\n",
    "\n",
    "sql_geography = \"\"\"\n",
    "CREATE TABLE census_zipcode_geography (\n",
    "  USPS CHAR(5) PRIMARY KEY,  -- by implication, UNIQUE NOT NULL\n",
    "  ALAND BIGINT,              -- some zips are larger than 2e9 m^2\n",
    "  AWATER BIGINT,\n",
    "  ALAND_SQMI NUMERIC(8, 3),  -- largest zips need 5 to left of decimal\n",
    "  AWATER_SQMI NUMERIC(8, 3), -- sizes with 3 digits of precision\n",
    "  INTPTLAT REAL,             -- keep fields from key, although duplicative\n",
    "  INTPTLONG REAL,\n",
    "  location POINT             -- use geometric type for lat/lon\n",
    ");\n",
    "\"\"\"\n",
    "cur_pg8k.execute(sql_geography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stipulate that this data is large enough we do not want to load it all at once (really it is not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ('USPS', 'ALAND', 'AWATER', 'ALAND_SQMI', \n",
    "          'AWATER_SQMI', 'INTPTLAT', 'INTPTLONG', 'location')\n",
    "\n",
    "sql_insert_geo = f\"\"\"\n",
    "INSERT into census_zipcode_geography ({','.join(fields)})\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\n",
    "\"\"\"\n",
    "with open('../data/census-zipcodes-2018.tsv') as fh:\n",
    "    next(fh)   # discard header line\n",
    "    for line in fh:\n",
    "        row = line.strip().split('\\t')\n",
    "        row.append(f\"({row[-2]}, {row[-1]})\")\n",
    "        cur_pg8k.execute(sql_insert_geo, tuple(row))\n",
    "\n",
    "conn_pg8000.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see that our data is in the database, and as a preview of the POINT data type, let us make a query for the land area of those zipcodes that are close to where I live.  Unfortunately, the percent-sign format codes do not work inside a where clause, only in the VALUES.  We can interpolate manually though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['04443', Decimal('137.384'), Decimal('8.674'), '(45.227773,-69.353261)'],\n",
       " ['04479', Decimal('38.441'), Decimal('1.359'), '(45.124208,-69.287058)'],\n",
       " ['04930', Decimal('53.969'), Decimal('2.373'), '(45.027572,-69.31797)'],\n",
       " ['04939', Decimal('37.670'), Decimal('0.269'), '(45.077258,-69.158771)'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_near = \"\"\"\n",
    "SELECT USPS, ALAND_SQMI, AWATER_SQMI, location \n",
    "FROM census_zipcode_geography \n",
    "WHERE location <@ circle '((%f, %f), 0.15)';\n",
    "\"\"\"\n",
    "cur_pg8k.execute(sql_near % (45.1, -69.3))\n",
    "cur_pg8k.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for working with another adapter, let us close the connection we created with `pg8000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_pg8000.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous access\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)\n",
    "\n",
    "On modern computers, I/O is by far the slowest component.  Thread switches, let alone process switches, are relatively expensive.  Simply checking whether a given I/O operation is ready to provide more data is one or two orders of magnitude cheaper, and has zero memory cost compared to allocating a thread.  \n",
    "\n",
    "Using `aiopg` or `asyncpg` allows your program to perform other work while waiting for the results to arrive from a query.  However, doing so *does* require becoming familiar with the `await` and `async` keywords, and generally shifting your thinking towards the styles of programming required by `asyncio` in the standard library.  If speed of many simultaneous operations becomes even more imperative, using the third-party `uvloop` instead of `asyncio` can speed things up still more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple examples in this lesson will not come remotely close to those where any of this matters.  But for much larger datasets, and for multi-tenancy of RDBMS access, the differences can be huge.\n",
    "\n",
    "We first will import the `asyncio` scaffolding and the `aiopg` module.  Because `asyncio` does not claim to follow the DB-API, the module attributes like `.apilevel`, and `.paramstyle` do not exist.  However, *most* of the DB-API is still consistent; e.g. `.connect()`, `.cursor()`, `.execute()`, `.fetchall()` still have their familiar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from asyncio import get_event_loop, gather, as_completed\n",
    "import aiopg, asyncpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this code is running inside a Jupyter notebook which already has its own `asyncio` event loop, we need to use a third-party module called `nest_asyncio` to path the event loop and run async code in cells.  Outside of environments (Jupyter, web servers, GUI applications) that might create their own event loops, this is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to check the zip codes near certain latitude/longitude locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"latitude longitude\")\n",
    "locs = [Location(40.0, -105.3), Location(45.1, -69.3), \n",
    "        Location(34.9, -82.4), Location(42.6, -72.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Aiopg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an asyncronous adapter, we need to wrap our operation in a special coroutine function that is defined with `async def` rather than plain `def`.  Each of the steps has an extra `await` keyword to indicate that the event loop is free to do other work between each such line.  The logic, however, is very much the same as we have seen with other adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def near_location(loc):\n",
    "    conn = await aiopg.connect(**login._asdict())\n",
    "    cur = await conn.cursor()\n",
    "    await cur.execute(sql_near % loc)\n",
    "    results = await cur.fetchall()\n",
    "    return (loc, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot simply run this function, but need instead to tell the event loop to manage it.  In fact, let us let the event loop handle several such coroutines, each for a different reference location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(latitude=40.0, longitude=-105.3) ... 10 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('80025', Decimal('11.722'), Decimal('0.004'), '(39.939848,-105.283942)')\n",
      "('80027', Decimal('19.462'), Decimal('0.196'), '(39.950796,-105.159688)')\n",
      "\n",
      "Location(latitude=45.1, longitude=-69.3) ... 4 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('04443', Decimal('137.384'), Decimal('8.674'), '(45.227773,-69.353261)')\n",
      "('04479', Decimal('38.441'), Decimal('1.359'), '(45.124208,-69.287058)')\n",
      "\n",
      "Location(latitude=34.9, longitude=-82.4) ... 12 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('29601', Decimal('4.280'), Decimal('0.024'), '(34.847112,-82.402264)')\n",
      "('29605', Decimal('25.579'), Decimal('0.175'), '(34.774425,-82.37661)')\n",
      "\n",
      "Location(latitude=42.6, longitude=-72.5) ... 13 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('01054', Decimal('22.790'), Decimal('0.164'), '(42.468898,-72.484579)')\n",
      "('01301', Decimal('25.494'), Decimal('0.541'), '(42.626761,-72.60153)')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aws = [near_location(loc) for loc in locs]\n",
    "\n",
    "loop = get_event_loop()\n",
    "for ret in loop.run_until_complete(gather(*aws)):\n",
    "    loc, results = ret\n",
    "    print(loc, \"...\", len(results), \"tuples\\n\", \"-\"*70)\n",
    "    for tup in results[:2]:\n",
    "        print(tup)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has two limitations.  \n",
    "\n",
    "* Each time a new coroutine is created, it makes a new connection.  A more efficient approach is to create a *connection pool* and share connections as they are requested (but not close them implicitly at function end).\n",
    "* We wait for all the results from the various coroutines to be complete in `loop.run_until_complete()`.  If the 4th query is ready early, we cannot process it while waiting for the 1st query to complete.\n",
    "\n",
    "As a secondary concern, by doing a `.fetchall()` on the results, we cannot not process each result tuple immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def one_location(pool, loc):\n",
    "    async with pool.acquire() as conn:\n",
    "        async with conn.cursor() as cur:\n",
    "            await cur.execute(sql_near % loc)\n",
    "            results = []\n",
    "            async for row in cur:\n",
    "                # might process each tuple as soon as received\n",
    "                results.append(row)\n",
    "    return (loc, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def near_locations(locs):\n",
    "    async with aiopg.create_pool(**login._asdict(), maxsize=10) as pool:\n",
    "        queries = [one_location(pool, loc) for loc in locs]\n",
    "        for future in as_completed(queries):\n",
    "            loc, results = await future\n",
    "            print(loc, \"...\", len(results), \"tuples\\n\", \"-\"*70)\n",
    "            for tup in results[:2]:\n",
    "                print(tup)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(latitude=42.6, longitude=-72.5) ... 13 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('01054', Decimal('22.790'), Decimal('0.164'), '(42.468898,-72.484579)')\n",
      "('01301', Decimal('25.494'), Decimal('0.541'), '(42.626761,-72.60153)')\n",
      "\n",
      "Location(latitude=34.9, longitude=-82.4) ... 12 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('29601', Decimal('4.280'), Decimal('0.024'), '(34.847112,-82.402264)')\n",
      "('29605', Decimal('25.579'), Decimal('0.175'), '(34.774425,-82.37661)')\n",
      "\n",
      "Location(latitude=45.1, longitude=-69.3) ... 4 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('04443', Decimal('137.384'), Decimal('8.674'), '(45.227773,-69.353261)')\n",
      "('04479', Decimal('38.441'), Decimal('1.359'), '(45.124208,-69.287058)')\n",
      "\n",
      "Location(latitude=40.0, longitude=-105.3) ... 10 tuples\n",
      " ----------------------------------------------------------------------\n",
      "('80025', Decimal('11.722'), Decimal('0.004'), '(39.939848,-105.283942)')\n",
      "('80027', Decimal('19.462'), Decimal('0.196'), '(39.950796,-105.159688)')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loop = asyncio.get_event_loop()  \n",
    "loop.run_until_complete(near_locations(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important issue to keep in mind with `aiopg` is that is always sets AUTOCOMMIT to ON.  This means that every time an INSERT or DELETE or UPDATE command is executed, it is as if a `.commit()` was called immediately afterward.  Some of the transactional options are lost with this limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Asyncpg\n",
    "\n",
    "Using `asyncpg` can both be much faster than other adapters under high-volume, highly concurrent, access, and also is more accurate in capturing PostgreSQL features.  It *does* require jumping into the asyncronous style of programming, which is often less familiar.  Moreover, generally `asyncpg` *simplifies* the DB-API rather than following it.\n",
    "\n",
    "Let us write another short program to find records *near* certain latitude and longitude points as we did above.  In this case, we push the asynchronous style slightly more.  Rather than return entire queries, we will add each individual row to a shared queue; here we use a set, but a `collections.deque` or `asyncio.Queue` would be good choices also (probably better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "zipcodes = set()\n",
    "\n",
    "async def print_from_queue(q, early_stop=sys.maxsize):\n",
    "    while True:\n",
    "        while q:\n",
    "            row = q.pop()\n",
    "            print(f\"{row['usps']} | {row['location']} | {row['aland_sqmi']}\")\n",
    "            early_stop -= 1\n",
    "            if early_stop == 0:\n",
    "                return\n",
    "        await asyncio.sleep(0.1)\n",
    "        if not zipcodes:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coroutine `print_from_queue` can be managed by an event loop.  It will terminate if it does not have any new data for 1/10th of a second, or also possibly if some limit is set to make it stop early (the latter mostly only to limit output for this lesson).  Notice that a record returned by `asyncpg` is a custom data type that lets us index by the column names.  This setup is more interesting if we peform actual processing on each record rather than only printing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to populate the queue as well.  Here we make a connection (with an `await`).  Then we do our query within a transaction.  Since it is a SELECT rather than an UPDATE/INSERT/DELETE, the transaction makes no difference other than an opportunity to show the usage. The transaction can by wrapped in a context manager, but the connection cannot in `asyncpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29661 | asyncpg.pgproto.types.Point((35.016915, -82.491086)) | 68.860\n",
      "29609 | asyncpg.pgproto.types.Point((34.912592, -82.38817)) | 24.159\n",
      "01360 | asyncpg.pgproto.types.Point((42.677091, -72.453876)) | 34.263\n",
      "01347 | asyncpg.pgproto.types.Point((42.559294, -72.518753)) | 0.579\n",
      "80027 | asyncpg.pgproto.types.Point((39.950796, -105.159688)) | 19.462\n",
      "29601 | asyncpg.pgproto.types.Point((34.847112, -82.402264)) | 4.280\n",
      "04939 | asyncpg.pgproto.types.Point((45.077258, -69.158771)) | 37.670\n",
      "29615 | asyncpg.pgproto.types.Point((34.856825, -82.296139)) | 19.189\n",
      "04479 | asyncpg.pgproto.types.Point((45.124208, -69.287058)) | 38.441\n",
      "01375 | asyncpg.pgproto.types.Point((42.466691, -72.546751)) | 14.221\n"
     ]
    }
   ],
   "source": [
    "async def near_many(locs, zipcodes):\n",
    "    conn = await asyncpg.connect(**login._asdict())\n",
    "    for loc in locs:\n",
    "        async with conn.transaction():\n",
    "            for row in await conn.fetch(sql_near  % loc):\n",
    "                zipcodes.add(row)\n",
    "    await conn.close()\n",
    "\n",
    "loop = get_event_loop()\n",
    "loop.run_until_complete(near_many(locs, zipcodes))\n",
    "loop.run_until_complete(print_from_queue(zipcodes, early_stop=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at one of those Record objects we get from the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Record usps='01301' aland_sqmi=Decimal('25.494') awater_sqmi=Decimal('0.541') location=asyncpg.pgproto.types.Point((42.626761, -72.60153))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcodes.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another flexible ability with `asyncpq` is choosing the encoding of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'username': 'Alice', 'created_on': '2020-11-30T16:27:30.115556'}\n",
      "<class 'dict'> {'username': 'Bob', 'created_on': '2020-11-30T16:27:30.116392'}\n",
      "<class 'dict'> {'username': 'Carlos', 'created_on': '2020-11-30T16:27:30.11666'}\n",
      "<class 'dict'> {'username': 'Sybil', 'created_on': '2020-11-30T16:27:30.207111'}\n",
      "<class 'dict'> {'username': 'Trudy', 'created_on': '2020-11-30T16:27:30.207111'}\n",
      "<class 'dict'> {'username': 'Vanna', 'created_on': '2020-11-30T16:27:30.207111'}\n"
     ]
    }
   ],
   "source": [
    "from json import dumps, loads\n",
    "\n",
    "async def get_users():\n",
    "    sql = \"SELECT row_to_json(t) basic FROM (SELECT username, created_on FROM users) t;\"\n",
    "    conn = await asyncpg.connect(**login._asdict())\n",
    "    try:\n",
    "        await conn.set_type_codec('json', \n",
    "                                  encoder=dumps, decoder=loads, \n",
    "                                  schema='pg_catalog')\n",
    "        for row in await conn.fetch(sql):\n",
    "            print(type(row['basic']), row['basic'])\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(get_users())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, we are probably better off simply creating a Python dict from the dictionary-like `Record` object used by `asyncpg` though.  For example, this will preserve the datetime data type, whereas round-tripping through JSON will lose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'Alice', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 115556)}\n",
      "{'username': 'Bob', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 116392)}\n",
      "{'username': 'Carlos', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 116660)}\n",
      "{'username': 'Sybil', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 207111)}\n",
      "{'username': 'Trudy', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 207111)}\n",
      "{'username': 'Vanna', 'created_on': datetime.datetime(2020, 11, 30, 16, 27, 30, 207111)}\n"
     ]
    }
   ],
   "source": [
    "async def get_users2():\n",
    "    sql = \"SELECT username, created_on FROM users;\"\n",
    "    conn = await asyncpg.connect(**login._asdict())\n",
    "    try:\n",
    "        for row in await conn.fetch(sql):\n",
    "            print(dict(row))\n",
    "    finally:\n",
    "        await conn.close()\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(get_users2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/98619088-44ab6000-22e1-11eb-8f6d-5532e68ab274.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For users who want to venture beyond the standard `psycopg2` (or in the future `psycopg3`) adapter, several other good options are available.  For heavy workloads, using one of the asynchronous adapters can be a big win.  The speedup with `asyncpg` is greater than with `aiopg`, but at the cost of more distance from DB-API 2.0 conventions.\n",
    "\n",
    "In the future, async support is also planned for `psycopg3`, but at the time of this lesson, it is too early to guess how performance will compare between future options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
