{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Serialization with Python\n",
    "\n",
    "This course aims at an intermediate level Python programmer.  We will assume you already have some familiarity with basic Python constructs like importing libraries, writing classes, variables, flow control, and similar topics an introductory course will have addressed.  However, we do not assume an advanced level of knowledge, nor do we assume experience with the specific tools and libraries these lessons will address."
   ],
   "id": "9a18772be65db177"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Why Serialization?\n",
    "\n",
    "There are several key purposes for which data serialization, and correspondingly de-serialization, are important.  In one case, we simply wish to be able to persist data from a running Python program to resume computation later on, still utilizing the same data.  At other times, we would like to exchange data with other tools and programs, perhaps ones written in other programming languages altogether.  Data, of course, comes in many shapes and sizes, and can be utilized for many purposes."
   ],
   "id": "14aa7c438f374750"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pickles\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/91055530-28a9f380-e5fb-11ea-9c4a-237ae8275b48.jpg\" align=\"right\" width=\"33%\"/>In these lessons we will look first at the pickle module of the Python standard library.  For most purposes within the Python ecosystem itself, pickle is your go-to serialization technique."
   ],
   "id": "38ccc8128f3794af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Nested Data\n",
    "\n",
    "In the next several lessons, we will look at JSON, the Javascript Object Notation, which is widely used as a means of exchanging data among various programming languages and tools, in particular often associated with web services."
   ],
   "id": "5141759df0b068ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Flat Data\n",
    "\n",
    "After looking at the structured and hierachical data often represented in JSON, we look at working with CSV and other delimited files, which are the most common means of representing tabular data and sharing it among different tools.  For the CSV lessons, we consider both the Python standard library and the popular third party library Pandas."
   ],
   "id": "5f7ad6453c1e6fd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Document Data\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/91055568-36f80f80-e5fb-11ea-80e7-31bbc6c3d03c.png\" align=\"right\" width=\"33%\"/>Moving further into the world of typically document-oriented data, we look at several Python libraries for working with XML data sources.  A large share of all documents in the world live as XML, and Python contains excellent tools for accessing those."
   ],
   "id": "f971fe962c33b27d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/7065401/91055601-44ad9500-e5fb-11ea-82d8-77323dc0e4f0.jpg\" align=\"right\" width=\"33%\"/>\n",
    "\n",
    "# Numeric Datasets\n",
    "\n",
    "In the final two lessons, we will look at formats specifically concerned with scientific and numeric data, often including very large datasets.  The NumPy array library and the HDF5 storage format are both discussed."
   ],
   "id": "73a63a26e1c3a6ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Universal Python Serialization\n",
    "\n",
    "The first tool you should always think about when serializing Python objects is the native pickle format. A pickle can serialize *almost any* Python object in a binary format."
   ],
   "id": "e9acaa27c7b7bdc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "More specialized protocols exist for serialization within a cluster computing frameworks. Cloudpickle is widely used for this purpose, but is not specifically discussed in this training.  Later in this training we will look at a variety of formats that Python can work with, but that are not specific to Python objects.",
   "id": "92e89c09d32be6ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us start out by loading a few Python standard library modules this lesson will utilize.",
   "id": "38d7442beacafe01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:16:33.840514Z",
     "start_time": "2025-07-16T09:16:33.837052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import io\n",
    "from pprint import pprint\n",
    "from dataclasses import dataclass\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime"
   ],
   "id": "6fa7ac7567e17f18",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### A Byte Representation\n",
    "\n",
    "Let us create a dictionary and use pickle to serialize it in a binary form."
   ],
   "id": "1c080dd0253631f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:16:35.672678Z",
     "start_time": "2025-07-16T09:16:35.666598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_data = dict(name=\"David\", real_number=76.54, count=22,\n",
    "               pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])\n",
    "\n",
    "pkl = pickle.dumps(my_data)\n",
    "pprint(pkl, width=50)"
   ],
   "id": "9efbe0bbe1b3fb07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'\\x80\\x04\\x95g\\x00\\x00\\x00\\x00\\x00\\x00\\x00}'\n",
      " b'\\x94(\\x8c\\x04name\\x94\\x8c\\x05David'\n",
      " b'\\x94\\x8c\\x0breal_number\\x94G@S\"\\x8f\\\\(\\xf5\\xc3'\n",
      " b'\\x8c\\x05count\\x94K\\x16\\x8c\\x04pets\\x94]\\x94('\n",
      " b'\\x8c\\x08Astrophe\\x94\\x8c\\x07Kachina'\n",
      " b'\\x94\\x8c\\x07Jackson\\x94\\x8c\\x05Rebel\\x94eu.')\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Same Values, Different Object\n",
    "\n",
    "Unpickling a serialization will create an *equivalent* object, but not an identical object.  It should not be confused with a shared memory or concurrency mechanism (pickles are a building block for *some* concurrency models, however)."
   ],
   "id": "a4d7a5851c650b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:16:37.984869Z",
     "start_time": "2025-07-16T09:16:37.979049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = pickle.loads(pkl)\n",
    "new_data"
   ],
   "id": "b08a95aa9c6bfeac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'David',\n",
       " 'real_number': 76.54,\n",
       " 'count': 22,\n",
       " 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:16:40.287303Z",
     "start_time": "2025-07-16T09:16:40.283476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Equality:\", new_data == my_data)\n",
    "print(\"Identity:\", new_data is my_data)"
   ],
   "id": "3e28a6225bc88d1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality: True\n",
      "Identity: False\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pickling to Files\n",
    "\n",
    "We can pickle to raw bytes, but for many or most purposes, it is useful to write these serializations to files.  The functions `.dump()` and `load()` serialize to file-like objects rather than create byte strings."
   ],
   "id": "557970fbbfca3e4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:18:18.756149Z",
     "start_time": "2025-07-16T09:18:18.749694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/data.pkl', 'wb') as fh:\n",
    "    pickle.dump(my_data, fh)"
   ],
   "id": "ac5c8ec34e1743fb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:18:27.059970Z",
     "start_time": "2025-07-16T09:18:27.029770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "hexdump -C data.pkl"
   ],
   "id": "34e05357fe90892b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000  80 04 95 67 00 00 00 00  00 00 00 7d 94 28 8c 04  |...g.......}.(..|\n",
      "00000010  6e 61 6d 65 94 8c 05 44  61 76 69 64 94 8c 0b 72  |name...David...r|\n",
      "00000020  65 61 6c 5f 6e 75 6d 62  65 72 94 47 40 53 22 8f  |eal_number.G@S\".|\n",
      "00000030  5c 28 f5 c3 8c 05 63 6f  75 6e 74 94 4b 16 8c 04  |\\(....count.K...|\n",
      "00000040  70 65 74 73 94 5d 94 28  8c 08 41 73 74 72 6f 70  |pets.].(..Astrop|\n",
      "00000050  68 65 94 8c 07 4b 61 63  68 69 6e 61 94 8c 07 4a  |he...Kachina...J|\n",
      "00000060  61 63 6b 73 6f 6e 94 8c  05 52 65 62 65 6c 94 65  |ackson...Rebel.e|\n",
      "00000070  75 2e                                             |u.|\n",
      "00000072\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reading Objects from Files\n",
    "\n",
    "Reading a pickle from a file—or from another file-like object—is exactly symmetrical with writing it.  With Python's so-called duck-typing, anything with a `.read()` method producing bytes allows unpickling.  Symmetrically, any object with a `.write()` method accepting bytes is suitable for pickling."
   ],
   "id": "4253b470665784de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:59:18.968526Z",
     "start_time": "2025-07-16T09:59:18.947217Z"
    }
   },
   "cell_type": "code",
   "source": "pickle.load(open('data/data.pkl', 'rb'))",
   "id": "101db33cf04f7c0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'David',\n",
       " 'real_number': 76.54,\n",
       " 'count': 22,\n",
       " 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### File-Like Objects\n",
    "\n",
    "A regular file on the local filesystem is a common location for pickles, but they might be available over a socket, or from a database connection with BLOB storage of pickles, or over an HTTP request, and so on.  For example, perhaps a zip file contains one or more pickles."
   ],
   "id": "c53e1d9deafb0f81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:32:24.353927Z",
     "start_time": "2025-07-16T09:32:24.306967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "zip data data.pkl"
   ],
   "id": "690ae987f1780158",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: data.pkl (deflated 5%)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:32:48.530284Z",
     "start_time": "2025-07-16T09:32:48.508511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with ZipFile('data/data.zip') as zf:\n",
    "    with zf.open('data.pkl') as zfile:\n",
    "        pprint(pickle.load(zfile))"
   ],
   "id": "d206bc74ae015979",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 22,\n",
      " 'name': 'David',\n",
      " 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel'],\n",
      " 'real_number': 76.54}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Other File-Like Objects\n",
    "\n",
    "Python uses *duck-typing* quite extensively; a great many things are file-like.  For example, we might use a memory IO buffer as the equivalent of a file."
   ],
   "id": "b91d99f9a69b7e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:35:15.033798Z",
     "start_time": "2025-07-16T09:35:15.019290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memfile = io.BytesIO(pkl)\n",
    "pickle.load(memfile)"
   ],
   "id": "97aee3ec270a686a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'David',\n",
       " 'real_number': 76.54,\n",
       " 'count': 22,\n",
       " 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pickle Limitations\n",
    "\n",
    "Most Python objects can be pickled and unpickled.  A simple dictionary, with some scalars and one nested list, were used in the examples earlier.  You are not limited default data structures; however, there are a few limits.  If you pickle an instance of a class, the class itself needs to be available on the receiving system.  Often this is no problem, since the class is from a library installed at both ends."
   ],
   "id": "481ca6e60cf987a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Round-Trip with a DataClass\n",
    "\n",
    "Perhaps we wish to use a dataclass instead of a dictionary in a program."
   ],
   "id": "785a320ac06e8b2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:36:38.878281Z",
     "start_time": "2025-07-16T09:36:38.850575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Trainer:\n",
    "    name: str\n",
    "    real_number: float\n",
    "    count: int\n",
    "    pets: list"
   ],
   "id": "e093d10ea8a40805",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:36:53.438168Z",
     "start_time": "2025-07-16T09:36:53.428533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_instance = Trainer(name=\"David\", real_number=76.54, count=22,\n",
    "                      pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])"
   ],
   "id": "7ee6bdfc9b02bf4b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:37:03.673655Z",
     "start_time": "2025-07-16T09:37:03.659153Z"
    }
   },
   "cell_type": "code",
   "source": "my_instance",
   "id": "da7f5033132930a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trainer(name='David', real_number=76.54, count=22, pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:37:45.903161Z",
     "start_time": "2025-07-16T09:37:45.893240Z"
    }
   },
   "cell_type": "code",
   "source": "pickle.loads(pickle.dumps(my_instance))",
   "id": "7323c7a15103cd7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trainer(name='David', real_number=76.54, count=22, pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Round-Trip with Datetimes\n",
    "\n",
    "Or we want to store and retrieve datetime values."
   ],
   "id": "99572fcb884dce8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:38:21.162778Z",
     "start_time": "2025-07-16T09:38:21.147812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events = {'description': 'Developed Lesson',\n",
    "          'start': datetime.fromisoformat('2020-05-22T12:11:10'),\n",
    "          'end': datetime(2020, 5, 23, 9, 10, 11)}\n",
    "events"
   ],
   "id": "43bf3a50fc5a3ca4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Developed Lesson',\n",
       " 'start': datetime.datetime(2020, 5, 22, 12, 11, 10),\n",
       " 'end': datetime.datetime(2020, 5, 23, 9, 10, 11)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:38:51.778583Z",
     "start_time": "2025-07-16T09:38:51.737576Z"
    }
   },
   "cell_type": "code",
   "source": "pickle.loads(pickle.dumps(events))",
   "id": "6bee0fa733c44ad6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Developed Lesson',\n",
       " 'start': datetime.datetime(2020, 5, 22, 12, 11, 10),\n",
       " 'end': datetime.datetime(2020, 5, 23, 9, 10, 11)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Classes\n",
    "\n",
    "If the class we want is not available, or even simply lives in the wrong namespace, we will not succeed in unpickling.  For example, a pickle file is available, but the code defining the class of the pickled instance is not on the local system."
   ],
   "id": "d5f14b7f0706cae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the files associated with this course",
   "id": "f4264f751332c528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:39:44.852049Z",
     "start_time": "2025-07-16T09:39:44.833579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with open('data/3dpoint.pkl', 'rb') as fh:\n",
    "        print(pickle.load(fh))\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "9427282827ba06a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '3dpoint.pkl'\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transient State\n",
    "\n",
    "Pickling is not directly possible for objects that are inherently impermanent.  For example, objects may represent file descriptors to the local filesystem or connections to a database.  The store the state of one particular computer at one particular time, and cannot be serialized."
   ],
   "id": "b7332ef2fcac8c55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:41:08.575790Z",
     "start_time": "2025-07-16T09:41:08.568154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hello = \"¡Hola Mundo!\"\n",
    "num = 999\n",
    "fname = 'data/test.data'\n",
    "fd = open(fname, 'w')\n",
    "fd.write(hello)\n",
    "data = {'fd': fd, 'num': num, 'hello': hello}"
   ],
   "id": "f370bdaa39ce51c6",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T09:47:43.086799Z",
     "start_time": "2025-07-16T09:47:43.075092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    pickle.dumps(data)\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "dcc1721f3753e217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot pickle 'TextIOWrapper' instances\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Customizing Serialization\n",
    "\n",
    "If you wish to serialize and deserialize classes you create yourself, you are free to specify which data is actually necessary and relevant for recreating *equivalent* instances.  This customization can allow you to initialize transient state in manner to allow something close to round-tripping.  For example, a particular local file cannot be shared on a non-networked filesystem; however, unpickling might create a usable file local to the destination filesystem."
   ],
   "id": "dca2b46b52a8a485"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:13:43.167326Z",
     "start_time": "2025-07-16T10:13:43.158835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HelloNumber:\n",
    "    \"Plain class that holds file descriptor\"\n",
    "    def __init__(self, fname, hello, num):\n",
    "        self.fd = open(fname, 'w+')\n",
    "        self.fd.write(hello)\n",
    "        self.num = num\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"<{self.__class__.__name__} holding file \"\n",
    "                f\"{self.fd.name}({self.fd.fileno()}) and num {self.num}>\")"
   ],
   "id": "c9e720293af227ab",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can add the capability of serializing the most important information in an instance to a simple tuple (another structure would work also; e.g. a list, a dict, a namedtuple, etc).",
   "id": "f558f0f0dc2ed565"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:13:44.579256Z",
     "start_time": "2025-07-16T10:13:44.573170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HelloNumber2(HelloNumber):\n",
    "    \"Add the ability to pickle the essential data\"\n",
    "    def __getstate__(self):\n",
    "        pos = self.fd.tell()\n",
    "        self.fd.flush()\n",
    "        self.fd.seek(0)\n",
    "        hello = self.fd.read()\n",
    "        self.fd.seek(pos)\n",
    "        data = (self.fd.name, # fname\n",
    "                hello,   # file content\n",
    "                self.num)\n",
    "        print(\"Pickling tuple only...\")\n",
    "        return data"
   ],
   "id": "4854a5da9634149a",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `.__init__()` of a class is not called during unpickling.  By default its `.__dict__` is simply restored.  We can make our class do something different from that.",
   "id": "aacb82efc6af293b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:13:46.329449Z",
     "start_time": "2025-07-16T10:13:46.325477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HelloNumber3(HelloNumber2):\n",
    "    \"Add the ability to reconstruct local state\"\n",
    "    def __setstate__(self, data):\n",
    "        self.fd = open(fname, 'w+')\n",
    "        self.fd.write(data[1])\n",
    "        self.num = data[2]"
   ],
   "id": "1b486e59de94b982",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us create an instance then round-trip it.",
   "id": "f76e2bc73f96e440"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:13:48.137222Z",
     "start_time": "2025-07-16T10:13:48.124516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hi = HelloNumber3(fname, hello, num)\n",
    "print(hi)"
   ],
   "id": "bb14c566c15e4a00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HelloNumber3 holding file test(81) and num 3>\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:13:50.989404Z",
     "start_time": "2025-07-16T10:13:50.982757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pkl = pickle.dumps(hi)\n",
    "pprint(pkl, width=60)"
   ],
   "id": "6ff2d5dc467cc9aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling tuple only...\n",
      "(b'\\x80\\x04\\x954\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x08__main_'\n",
      " b'_\\x94\\x8c\\x0cHelloNumber3\\x94\\x93\\x94)\\x81\\x94\\x8c\\x04'\n",
      " b'test\\x94\\x8c\\x05Hello\\x94K\\x03\\x87\\x94b.')\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:05:04.275062Z",
     "start_time": "2025-07-16T10:05:04.238332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_hi = pickle.loads(pkl)\n",
    "print(new_hi)"
   ],
   "id": "ed3b8146bb6c5f42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'David', 'real_number': 76.54, 'count': 22, 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reading and Writing JSON\n",
    "\n",
    "JavaScript Object Notation (JSON) is a widely used data exchange format.  As the name suggests, it is a format derived from JavaScript, but it is strictly language neutral. JSON is currently specified by Internet Engineering Task Force (IETF) RFC 8259.\n",
    "\n",
    "JSON is supported by a great many programming languages, in their standard library, or as built-ins, or with widely available libraries for those languages.  Many JSON strings are also identical to valid Python expression for some data structure or scalar."
   ],
   "id": "e468d58bfbb592d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:27:34.489715Z",
     "start_time": "2025-07-16T10:27:34.429932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from textwrap import fill\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "from fractions import Fraction\n",
    "from math import pi\n",
    "import jsonpickle"
   ],
   "id": "60b109aff88b803c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A String Representation\n",
    "\n",
    "Let us create a dictionary and use the `json` module to serialize it in a string form. The examples in this lesson will largely follow those used the the lesson on Python pickles."
   ],
   "id": "47e6d932aed9e8b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:29:58.844603Z",
     "start_time": "2025-07-16T10:29:58.831537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Being still alive, lifespan is unknown & marked with NaN\n",
    "my_data = dict(name=\"David\", real_number=76.54, count=22, likes_python=True,\n",
    "               lifespan=float('nan'), end_of_time=float('inf'),\n",
    "               pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])\n",
    "\n",
    "jstr =json.dumps(my_data)\n",
    "print(fill(jstr, width=65))"
   ],
   "id": "999702cb9d2c8530",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"David\", \"real_number\": 76.54, \"count\": 22,\n",
      "\"likes_python\": true, \"lifespan\": NaN, \"end_of_time\": Infinity,\n",
      "\"pets\": [\"Astrophe\", \"Kachina\", \"Jackson\", \"Rebel\"]}\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:30:30.345361Z",
     "start_time": "2025-07-16T10:30:30.334462Z"
    }
   },
   "cell_type": "code",
   "source": "print(fill(str(my_data), width=65))",
   "id": "711d3e528ce399f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'David', 'real_number': 76.54, 'count': 22,\n",
      "'likes_python': True, 'lifespan': nan, 'end_of_time': inf,\n",
      "'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Almost Just Python\n",
    "\n",
    "The JSON string representing the `my_data` dictionary is *almost* valid Python that we could copy-paste or `eval()`.  The main differences are the spelling different of `true` versus `True`, of `false` versus `False`, and of `null` versus `None`."
   ],
   "id": "69cf22bc575cf38f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another subtle issue occurred in the example, however.  The name `nan` is neither a Python keyword or built-in name *nor* is it strictly part of the JSON spec.  This special class of floating-point values (Not-a-Number) is very useful for certain numeric purposes, so many JSON libraries add it as an informal extension.",
   "id": "2929a9534d31513b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The JSON version is spelled `NaN`.  In Python, we could import the name `nan` from the `math` or `numpy` modules, or we can build it using the `float()` constructor.  The constants `+Infinity` and `-Infinity` which are part of the IEEE-754 floating point standard, likewise are often useful, but are not part of JSON narrowly.",
   "id": "e33610722d435061"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:44:00.483595Z",
     "start_time": "2025-07-16T10:44:00.467559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    json.dumps(my_data, allow_nan=False)\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "3506f1df02876bfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of range float values are not JSON compliant: nan\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Same Values, Different Object\n",
    "\n",
    "Serialization and deserialization will create an *equivalent* object, but not an identical object.  It should not be confused with a shared memory or concurrency mechanism (but serialization is a building block for *some* concurrency models)."
   ],
   "id": "471b4f8644ef5544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:49:00.686771Z",
     "start_time": "2025-07-16T10:49:00.680785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Avoid the NaN issue\n",
    "my_data = dict(name=\"David\", likes_python=True, count=None,\n",
    "               pets=['Astrophe', 'Kachina', 'Jackson', 'Rebel'])\n",
    "\n",
    "jstr =json.dumps(my_data)\n",
    "new_data = json.loads(jstr)"
   ],
   "id": "cac0548d83fcef5c",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:49:02.580943Z",
     "start_time": "2025-07-16T10:49:02.576199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Equality:\", new_data == my_data)\n",
    "print(\"Identity:\", new_data is my_data)"
   ],
   "id": "8709b83ccc91d1fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equality: True\n",
      "Identity: False\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Serializing JSON to Files\n",
    "\n",
    "The API of the `json` module generally matches that of `pickle`.  Along with the `dumps()` and `loads()`, the `json` module also has `dump()` and `load()`.  In all of these, the final 's' is a very compact way of expressing the idea that the function consumes or produces *strings* rather than files.  That naming convention has an old history; most likely newer methods that did not require backward compatibility would use more obvious names."
   ],
   "id": "d47c2d4816141ee2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In contrast to pickle format, which is *usually* used to save files with serialized objects, JSON is *usually* used to create an in-memory string to send over various wire protocols.",
   "id": "d10071d2922c655f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:49:06.048579Z",
     "start_time": "2025-07-16T10:49:05.899330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/data.json', 'w') as fh:\n",
    "    json.dump(my_data, fh)\n",
    "\n",
    "!cat tmp/data.json"
   ],
   "id": "24f28cdbf89e68d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: tmp/data.json: No such file or directory\r\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Reading Objects from Files\n",
    "\n",
    "Reading JSON from a file—or from another file-like object—is exactly symmetrical with writing it.  With Python's so-called duck-typing, anything with a `.read()` method producing bytes allows unpickling.  Symmetrically, any object with a `.write()` method accepting bytes is suitable for pickling.  See examples in the previous lesson for use of several file-like objects. In this respect, `pickle` and `json` functions are the same."
   ],
   "id": "ca6f86755e3cb594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:49:35.841937Z",
     "start_time": "2025-07-16T10:49:35.822562Z"
    }
   },
   "cell_type": "code",
   "source": "json.load(open('data/data.json'))",
   "id": "5d4a0618af3a5c50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'David',\n",
       " 'likes_python': True,\n",
       " 'count': None,\n",
       " 'pets': ['Astrophe', 'Kachina', 'Jackson', 'Rebel']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# JSON Limitations\n",
    "\n",
    "Only basic Python collections and scalars can be directly represented in JSON; however, these collections *can* be nested indefinitely.  Specifically, JSON allows for dictionaries (called \"objects\" in the spec) and lists (called \"arrays\" in the spec); JSON does not have a way of representing tuples, sets, `collections.deque`, `collections.Counter`, NumPy arrays, or other collections you might use in Python.  The keys for JSON objects may only be strings, unlike Python dictionaries that can use any hashable object. For many purposes, casting another collection to a list suffices to transmit the data."
   ],
   "id": "e7b627875119ab87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The scalars supported by JSON are exclusively: the three literal names `true`, `false`, and `null`, strings, and numbers.  Strings are surrounded by double quotes, and may contain escaped Unicode code points.  JSON itself only contains a generic \"number\" datatype.  By default, numbers without decimal points will be interpreted as Python ints.  Numbers with decimals will be interpreted as Python floats.  Python allows other number types, such as `decimal.Decimal`, `fraction.Fraction`, or NumPy values of specific bit lengths.",
   "id": "6da899026c255c3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Serialization Failures\n",
    "\n",
    "Any custom classes, including ones that represent special scalars, will fail by default in JSON serialization."
   ],
   "id": "ab310542b4bd0423"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:55:40.913637Z",
     "start_time": "2025-07-16T10:55:40.900835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "timestamp = datetime.fromisoformat('2020-05-24T00:55:10')\n",
    "try:\n",
    "    json.dumps(timestamp)\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "91a3cd66d2af4ec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object of type datetime is not JSON serializable\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:56:00.298424Z",
     "start_time": "2025-07-16T10:56:00.254790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decnum = Decimal('3.1415')\n",
    "try:\n",
    "    json.dumps(decnum)\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "7de10afd99a13594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object of type Decimal is not JSON serializable\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Forcing Serialization\n",
    "\n",
    "We can customize how special Python datatypes are serialized and deserialized.  This should be done with a caution, however, because it also can impact interoperability with other systems.  This might mean system in other programming languages, or it might simply be other Python machines without the same customizations.  First let us handle extra serialization."
   ],
   "id": "a3f6473f47d6cc78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:56:18.386197Z",
     "start_time": "2025-07-16T10:56:18.369281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScalarEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, (Decimal, Fraction)):\n",
    "            return float(o)\n",
    "        elif isinstance(o, datetime):\n",
    "            return datetime.isoformat(o)\n",
    "        else:\n",
    "            return super().default(o)"
   ],
   "id": "dd7f6fd6798d8399",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Semi-Generic Types\n",
    "\n",
    "Let us encode some data using the custom encoder we developed."
   ],
   "id": "230c34f20862e6cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T10:56:39.479848Z",
     "start_time": "2025-07-16T10:56:39.463852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nums = [timestamp, 42, decnum, pi, Fraction(22, 7)]\n",
    "jnums = json.dumps(nums, cls=ScalarEncoder)\n",
    "pprint(jnums, width=55)"
   ],
   "id": "270ea239ef2df528",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[\"2020-05-24T00:55:10\", 42, 3.1415, '\n",
      " '3.141592653589793, 3.142857142857143]')\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This customization will not introduce much compatibility concern.  The same \"number\" can be represented in different systems.  However, notice that in the JSON representation absolutely nothing distinguishes the float, Decimal, and Fraction we started with as several approximations of the transcendental number pi.  The timestamp has simply become a string, but one that contains all the underlying information.",
   "id": "ba86647ee9d0ca40"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reading back in this JSON serialization will work fine, but with all non-integral numbers as platform-native floats.  We can change the default deserialization type for floats and ints if we would like to. We impose just one type for each of float and int.",
   "id": "1be47aacd942fdc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:08:55.180166Z",
     "start_time": "2025-07-16T11:08:55.165371Z"
    }
   },
   "cell_type": "code",
   "source": "json.loads(jnums)",
   "id": "3c5112d29fae8aae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-24T00:55:10', 42, 3.1415, 3.141592653589793, 3.142857142857143]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:09:07.073506Z",
     "start_time": "2025-07-16T11:09:07.062881Z"
    }
   },
   "cell_type": "code",
   "source": "json.loads(jnums, parse_float=Decimal, parse_int=Fraction)",
   "id": "792c3a2c68aaef11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-24T00:55:10',\n",
       " Fraction(42, 1),\n",
       " Decimal('3.1415'),\n",
       " Decimal('3.141592653589793'),\n",
       " Decimal('3.142857142857143')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Customizing Serialization\n",
    "\n",
    "For complex objects, the `.__dict__` of the object often serves as a reasonable proxy for \"the interesting data\" inside the object. We saw a definition of a custom encoder and could enhance it to deal with additional types that way. However, this is about the point where you want to worry more about the actual utility of your serialization, especially if you will transmit it to other systems (i.e. running different programming languages)."
   ],
   "id": "94c6c80eb15f731d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:09:28.341691Z",
     "start_time": "2025-07-16T11:09:28.317211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RobustEncoder(ScalarEncoder):\n",
    "    def default(self, o):\n",
    "        try:\n",
    "            return super().default(o)\n",
    "        except:\n",
    "            return o.__dict__"
   ],
   "id": "364c93fa8a06154f",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:09:40.195003Z",
     "start_time": "2025-07-16T11:09:40.187086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class TestData:\n",
    "    description: str\n",
    "    timestamp: datetime\n",
    "    numbers: list"
   ],
   "id": "f60c3a657e81e3a1",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:09:45.963215Z",
     "start_time": "2025-07-16T11:09:45.956778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = TestData(description=\"Pi approximations\",\n",
    "                     timestamp=timestamp,\n",
    "                     numbers=[decnum, pi, Fraction(22, 7)])\n",
    "pprint(str(test_data), width=56)"
   ],
   "id": "63b31d56c97a7109",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"TestData(description='Pi approximations', \"\n",
      " 'timestamp=datetime.datetime(2020, 5, 24, 0, 55, 10), '\n",
      " \"numbers=[Decimal('3.1415'), 3.141592653589793, \"\n",
      " 'Fraction(22, 7)])')\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:10:29.492655Z",
     "start_time": "2025-07-16T11:10:29.478416Z"
    }
   },
   "cell_type": "code",
   "source": "pprint(json.dumps(test_data, cls=RobustEncoder))",
   "id": "6de885cf6c42849e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"description\": \"Pi approximations\", \"timestamp\": \"2020-05-24T00:55:10\", '\n",
      " '\"numbers\": [3.1415, 3.141592653589793, 3.142857142857143]}')\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# JSON Pickles\n",
    "\n",
    "If your concern for interoperability is low, and you only wish to exchange data between reasonably similarly configured Python systems (or only persist objects on the same system), the third-party module `jsonpickle` does this abstraction for you.  This achieves round-tripping, which is often useful.  Its capabilities and limitations are essentially identical to `pickle` itself.  However, the binary pickle format is considerably more compact than the JSON string format."
   ],
   "id": "a4ebfa91b7c75561"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:10:50.332850Z",
     "start_time": "2025-07-16T11:10:50.319190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "jpkl = jsonpickle.encode(test_data, indent=True)\n",
    "new_data = jsonpickle.decode(jpkl)\n",
    "pprint(str(new_data), width=56)"
   ],
   "id": "24f78b0a529d3ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"TestData(description='Pi approximations', \"\n",
      " 'timestamp=datetime.datetime(2020, 5, 24, 0, 55, 10), '\n",
      " \"numbers=[Decimal('3.1415'), 3.141592653589793, \"\n",
      " 'Fraction(22, 7)])')\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Verbose Format\n",
    "\n",
    "Above, I used the `indent=True` option to produce more human readable (but somewhat larger) JSON output.  It only modifies semantically meaningless whitespace.  The same switch exits on the `pickle` module itself.  Let us look at what is contained in this specialized JSON format. We will use several slides to see the parts."
   ],
   "id": "a7e9d1a69549c324"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:11:18.488612Z",
     "start_time": "2025-07-16T11:11:18.483747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lines = jpkl.splitlines()\n",
    "print('\\n'.join(lines[:14]))"
   ],
   "id": "e82717f8285e48f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"py/object\": \"__main__.TestData\",\n",
      " \"description\": \"Pi approximations\",\n",
      " \"timestamp\": {\n",
      "  \"py/object\": \"datetime.datetime\",\n",
      "  \"__reduce__\": [\n",
      "   {\n",
      "    \"py/type\": \"datetime.datetime\"\n",
      "   },\n",
      "   [\n",
      "    \"B+QFGAA3CgAAAA==\"\n",
      "   ]\n",
      "  ]\n",
      " },\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:11:28.900777Z",
     "start_time": "2025-07-16T11:11:28.897461Z"
    }
   },
   "cell_type": "code",
   "source": "print('\\n'.join(lines[14:28]))",
   "id": "5c0a60eb20306895",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"numbers\": [\n",
      "  {\n",
      "   \"py/reduce\": [\n",
      "    {\n",
      "     \"py/type\": \"decimal.Decimal\"\n",
      "    },\n",
      "    {\n",
      "     \"py/tuple\": [\n",
      "      \"3.1415\"\n",
      "     ]\n",
      "    }\n",
      "   ]\n",
      "  },\n",
      "  3.141592653589793,\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print('\\n'.join(lines[28:]))",
   "id": "f5feda40f94bf3dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sharing JSON Among Languages\n",
    "\n",
    "JavaScript Object Notation (JSON) is designed as a data interchange format.  Specifically, it is pobably used most commonly for RESTful web service (Representational state transfer).  While those might run in Python, there are numerous other programming languages and frameworks they might use; notably JavaScript is a prominent option.  Every widely used modern programming language has libraries supporting JSON."
   ],
   "id": "b2a32b754770f937"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For this lesson, we utilize an example Node.js server that is licensed as GPL v.3.0, and can be installed from Rob Kendal's GitHub repository at https://github.com/sereynha/ecommerce.  That repository is accompanied by an excellent introductory article that describes the steps of creating a simple Node.js webserver.  I have modified that code only in minor ways for this lesson.  I will show two snippets of the JavaScript code used for illustration, but the focus here is on talking to the server from Python, not learning JavaScript or Node.js.",
   "id": "ca8fb3cd0c6e7026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T11:56:29.193869Z",
     "start_time": "2025-07-16T11:56:29.010276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from http import HTTPStatus\n",
    "import requests\n",
    "!cp node-server/data/users-start.json node-server/data/users.json"
   ],
   "id": "bb3d1db5642ac547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: node-server/data/users-start.json: No such file or directory\r\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Making REST Requests\n",
    "\n",
    "This lesson—and *microservices* very commonly—will consist of calling a webserver with a *payload* formatted as JSON, and receiving a response, also usually formatted as JSON.  This structure allows many servers to interact in a manner similar to function calls, with both computation and state distributed among the various servers.  An older approach to this same architecture was XMLRPC, which in fact has a current but legacy Python standard library module `xmlrpc` to support it."
   ],
   "id": "22de182b4dc4c3f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The server in this lesson provides a simple key/value database of users.  All users must have a name and a password, but they may also optionally have other data associated with them.  This design is obviously terrible from a security perspective, since \"passwords\" are transmitted and stored without encryption (as is other data), but that concern is not for this lesson.",
   "id": "e63bfcde84bcf0c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The third-party package `requests` is recommended for HTTP clients, even in the Python standard library documentation itself.  However, the standard library package `urllib.request` has a less intuitive API, but will perform the same tasks if the third-party package is not available.  In our server, we can query the data it contains by making a GET request to the endpoint `/api/products`.",
   "id": "145c6df995cd48af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A GET request does not pass any JSON body data; in principle it could pass URL parameters to communicate data, but that style is not used in this lesson.",
   "id": "a58eaf4d46696882"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:34:22.259557Z",
     "start_time": "2025-07-16T12:34:22.126525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'http://localhost:3031/api/products?page=1&limit=5'\n",
    "\n",
    "login_url = 'http://localhost:3031/api/auth/login'\n",
    "login_data = {\n",
    "    'email': 'your_email@example.com',\n",
    "    'password': 'your_password'\n",
    "}\n",
    "\n",
    "login_response = requests.post(login_url, json=login_data)\n",
    "token = json.loads(login_response.text).get('token')\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {token}'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Show status code and load JSON body\n",
    "print(response.status_code)\n",
    "print(response.headers['Content-Type'])\n",
    "data = json.loads(response.text)\n",
    "print(data)"
   ],
   "id": "6751edcd1690b64a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "application/json; charset=utf-8\n",
      "{'count': 3, 'data': [{'id': 3, 'name': 'Beer', 'description': 'A sweet Beer', 'stock': 0, 'price': '15', 'tags': 'Berr,Thai', 'categoryId': 1, 'createdAt': '2024-07-28T15:25:25.919Z', 'updatedAt': '2024-07-28T15:25:25.919Z'}, {'id': 2, 'name': 'Knitted T-Shirt', 'description': 'Knitted t-shirt featuring short sleeves, graphic print at the front and crew neckline.60% Polyester 40% Viscose', 'stock': 0, 'price': '8.69', 'tags': 'T-Shirt,clothing', 'categoryId': 1, 'createdAt': '2024-07-28T11:12:11.298Z', 'updatedAt': '2024-07-28T11:12:11.298Z'}, {'id': 1, 'name': 'Cross Training T-Shirt', 'description': 'Model is 173 cm tall / 65 kg weight and is wearing size M.', 'stock': 6, 'price': '8.69', 'tags': 'T-Shirt,clothing,updated', 'categoryId': 1, 'createdAt': '2024-07-27T08:10:21.196Z', 'updatedAt': '2025-07-16T12:30:35.787Z'}]}\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Unsuccessful Requests\n",
    "\n",
    "A well behaving webserver will return a status code indicating the nature of the problem with a request. A very small support function will help us show the response details."
   ],
   "id": "2c132c43ad94a31e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:10:30.925304Z",
     "start_time": "2025-07-16T12:10:30.908286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def phrase(response):\n",
    "    for st in HTTPStatus:\n",
    "        if st.value == response.status_code:\n",
    "            return f\"{st.value} {st.phrase}\""
   ],
   "id": "9c727acf3b5cf174",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Trying a resource that simply does not exist.",
   "id": "c05cd54f32003ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:26:43.409458Z",
     "start_time": "2025-07-16T12:26:43.358823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url2 = 'http://localhost:3031/api/products/search/text?params=Beer'\n",
    "response = requests.get(url2, headers=headers)\n",
    "print(phrase(response))\n",
    "try:\n",
    "    json.loads(response.text)\n",
    "except Exception as err:\n",
    "    print(err)"
   ],
   "id": "bf9e26ee9477ad4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At times we might see a status code that is neither 200 nor 404.  A 404 will not have any body, but other status codes are likely to have a body that is encoded as plain text or in another manner.  We can use this clue to decide whether to JSON decode the body.",
   "id": "9872f94d663310bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:26:45.773560Z",
     "start_time": "2025-07-16T12:26:45.744851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url3 = 'http://localhost:3031/api/orders'\n",
    "response = requests.get(url3, headers=headers)\n",
    "print(phrase(response))\n",
    "print(response.headers['Content-Type'])\n",
    "response.text"
   ],
   "id": "444ad6280a65dc2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "application/json; charset=utf-8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"datas\":[]}'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pushing JSON\n",
    "\n",
    "The way this server is configured, the same endpoint behaves differently if it receives a POST request rather than a GET request.  With a POST, a new record is added to the database."
   ],
   "id": "1cfbf47b6348a3ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:30:35.831518Z",
     "start_time": "2025-07-16T12:30:35.690871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url4 = \"http://localhost:3031/api/carts\"\n",
    "cartData = {\n",
    "  \"productId\": 1,\n",
    "  \"quantity\": 2\n",
    "}\n",
    "response = requests.post(url4, json=cartData, headers=headers)\n",
    "print(phrase(response))\n",
    "response.text"
   ],
   "id": "d55d4e03b1c5cd17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 Created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"message\":\"Create successful\",\"success\":true}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us make sure the database has the contents we hope for.",
   "id": "fd58c935013a3947"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:34:46.005029Z",
     "start_time": "2025-07-16T12:34:45.939407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "json.loads(response.text)"
   ],
   "id": "399b27d42686ea52",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 3,\n",
       " 'data': [{'id': 3,\n",
       "   'name': 'Beer',\n",
       "   'description': 'A sweet Beer',\n",
       "   'stock': 0,\n",
       "   'price': '15',\n",
       "   'tags': 'Berr,Thai',\n",
       "   'categoryId': 1,\n",
       "   'createdAt': '2024-07-28T15:25:25.919Z',\n",
       "   'updatedAt': '2024-07-28T15:25:25.919Z'},\n",
       "  {'id': 2,\n",
       "   'name': 'Knitted T-Shirt',\n",
       "   'description': 'Knitted t-shirt featuring short sleeves, graphic print at the front and crew neckline.60% Polyester 40% Viscose',\n",
       "   'stock': 0,\n",
       "   'price': '8.69',\n",
       "   'tags': 'T-Shirt,clothing',\n",
       "   'categoryId': 1,\n",
       "   'createdAt': '2024-07-28T11:12:11.298Z',\n",
       "   'updatedAt': '2024-07-28T11:12:11.298Z'},\n",
       "  {'id': 1,\n",
       "   'name': 'Cross Training T-Shirt',\n",
       "   'description': 'Model is 173 cm tall / 65 kg weight and is wearing size M.',\n",
       "   'stock': 6,\n",
       "   'price': '8.69',\n",
       "   'tags': 'T-Shirt,clothing,updated',\n",
       "   'categoryId': 1,\n",
       "   'createdAt': '2024-07-27T08:10:21.196Z',\n",
       "   'updatedAt': '2025-07-16T12:30:35.787Z'}]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The server may validate a POST request (or any request) in some manner, and return an appropriate status based on the JSON passed to it.",
   "id": "3c6d47685fa9b0eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:36:20.701906Z",
     "start_time": "2025-07-16T12:36:20.641974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anon = {\"password\": \"P4cC!^*8chWz8\", \"profession\": \"Hacker\"}\n",
    "response = requests.post(url, data=json.dumps(anon), headers=headers)\n",
    "print(phrase(response))\n",
    "response.text"
   ],
   "id": "4fc3efcee5b08e67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 Unauthorized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"message\":\"Unauthorized\",\"errorCode\":401}'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# What the Server is Doing\n",
    "\n",
    "The Node.js server has a bit of scaffolding to implement a server.  A very similar webserver could be implemented in Python or any other programming language.  While you may not be familiar with JavaScript, the below code should not be difficult to understand in outline.  This is the code that handle a POST to the `/users` route."
   ],
   "id": "cd9262b33f34218c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Although the data file that stores the database is itself simply JSON, the server explicitly parses it as JSON to assure the format.  Setting the header immediately before the call to `res.send()` is redundant because the server can detect the type from the JSON object; I added it to illustrate that we are able to explicitly set it.  Very similar APIs are present in Python websevers.",
   "id": "cdb8c1cb1d597056"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# JSON Schema\n",
    "\n",
    "The prior lesson demonstrated communicating between a RESTful web server and a client.  Recall that we sent HTTP POST messages with a JSON body to a server and received JSON responses from GET queries.  One thing that was not done in the example was any validation of the format of these messages.  Or rather, there was one element of ad-hoc validation in that the server required the field \"name\" to be present in a user record."
   ],
   "id": "8aa5ffd794b1b765"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using JSON Schema, we can more precisely specify all the elements that may be present in an acceptable JSON document, including which are requires versus option, and indicate datatypes and nesting of containers.  JSON Schema can contain varying levels of details.  We will look at some possible schemata to define a valid user with varying degrees of specificity.",
   "id": "9def2486df5c4ca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us start out by loading Python standard library modules and the third-party `jsonschema` module.  We also create JSON strings for several users to validate.",
   "id": "73d4b72ae561639d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:39:18.243739Z",
     "start_time": "2025-07-16T12:39:18.097543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from jsonschema import validate, ValidationError"
   ],
   "id": "639cd766c376b8e2",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:39:34.112275Z",
     "start_time": "2025-07-16T12:39:34.105133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "guido = json.loads(\"\"\"{\n",
    "  \"name\": \"Guido van Rossum\",\n",
    "  \"password\": \"unladenswallow\",\n",
    "  \"details\": {\n",
    "    \"profession\": \"ex-BDFL\"\n",
    "  }\n",
    "}\"\"\")"
   ],
   "id": "4e68733f24ca1ac7",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:52:35.113651Z",
     "start_time": "2025-07-16T12:52:35.105634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "david = json.loads(\"\"\"{\n",
    "  \"name\": \"David Mertz\",\n",
    "  \"password\": \"badpassword\",\n",
    "  \"details\": {\n",
    "    \"profession\": \"Data Scientist\",\n",
    "    \"publisher\": \"INE\"\n",
    "  },\n",
    "  \"lucky_numbers\": [12, 42, 55, 87]\n",
    "}\"\"\")"
   ],
   "id": "a208c8454a55e704",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:52:45.087010Z",
     "start_time": "2025-07-16T12:52:45.073955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "intruder = json.loads(\"\"\"{\n",
    "  \"password\": \"P4cC!^*8chWz8\",\n",
    "  \"profession\": \"Hacker\"\n",
    "}\"\"\")"
   ],
   "id": "de43a7a48834d88",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Validation\n",
    "\n",
    "A JSON Schema is itself a JSON document following certain specifications.  At the simplest, it needs to specify a type for the JSON being validated. The module `jsonschema` expects Python objects as both `instance` and `schema` arguments.  If you are beginning with JSON—which is, after all, the point of using it—you need to use the `json` module to convert both to Python objects first."
   ],
   "id": "2bd6a5929bc95873"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The API the `jsonschema` module uses might be surprising.  It raises an exception on failure, but passes silently on success.  Let us look at a couple examples.",
   "id": "f8d90c73429289dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Checking Scalars",
   "id": "bbe4b120a8158b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:53:42.225360Z",
     "start_time": "2025-07-16T12:53:42.196220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    validate(instance=99, schema={\"type\": \"number\"})\n",
    "    print(\"99 is a number\")\n",
    "except ValidationError as err:\n",
    "    print(err)"
   ],
   "id": "4edf9bdc49911cb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 is a number\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T12:54:54.618742Z",
     "start_time": "2025-07-16T12:54:54.611254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    validate(99, {\"type\": \"string\"})\n",
    "    print(\"99 is a string\")\n",
    "except ValidationError as err:\n",
    "    print(err)"
   ],
   "id": "d227d74959a6db20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 is not of type 'string'\n",
      "\n",
      "Failed validating 'type' in schema:\n",
      "    {'type': 'string'}\n",
      "\n",
      "On instance:\n",
      "    99\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:21:12.484745Z",
     "start_time": "2025-07-16T13:21:12.467847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    validate(\"99\", {\"type\": \"number\"})\n",
    "    print(\"99 is a string\")\n",
    "except ValidationError as err:\n",
    "    print(err)"
   ],
   "id": "84872bd3770a5c2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'99' is not of type 'number'\n",
      "\n",
      "Failed validating 'type' in schema:\n",
      "    {'type': 'number'}\n",
      "\n",
      "On instance:\n",
      "    '99'\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A Test Function\n",
    "\n",
    "I find it easier to wrap the exception raising API with a function that will return either the error description as a string or None as a sentinel for \"no errors.\""
   ],
   "id": "21e36f38b621c9da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:23:26.891957Z",
     "start_time": "2025-07-16T13:23:26.873717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def not_valid(instance, schema):\n",
    "    try:\n",
    "        validate(instance, schema)\n",
    "        return None\n",
    "    except ValidationError as err:\n",
    "        return str(err)"
   ],
   "id": "7c7c53d7282fef78",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following is the pattern we will use for the remaining examples.",
   "id": "9ae13a22617ebf26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:23:42.393806Z",
     "start_time": "2025-07-16T13:23:42.378069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The \"walrus operator\" requires Python 3.8+\n",
    "if msg := not_valid(\"Ooops\", {\"type\": \"array\"}):\n",
    "    print(msg)"
   ],
   "id": "4ba510af61b30f14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ooops' is not of type 'array'\n",
      "\n",
      "Failed validating 'type' in schema:\n",
      "    {'type': 'array'}\n",
      "\n",
      "On instance:\n",
      "    'Ooops'\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Checking Users\n",
    "\n",
    "The simple examples above do not check structured collections. All user JSON records are what JavaScript calls \"objects\" but Python calls dicts.   For a JSON object, we need to define both the type and the properties we expect it to have.  We may specify keys as required, but validation will not prohibit inclusion of \"cargo\" in keys we have not specified.  Very often this is exactly desired behavior; JSON often carries extra information that might be used by other consumers, but a particular consumer only needs to assure the parts it cares about are present."
   ],
   "id": "522f564e6a88bcc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:24:19.160065Z",
     "start_time": "2025-07-16T13:24:19.153219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema = json.loads(\"\"\"{\n",
    "  \"type\" : \"object\",\n",
    "  \"required\": [\"name\"],\n",
    "  \"properties\" : {\n",
    "    \"name\" : {\"type\" : \"string\"}\n",
    "    }\n",
    "}\"\"\")"
   ],
   "id": "46ca77b908bd6f1",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:24:28.661311Z",
     "start_time": "2025-07-16T13:24:28.652319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for user in [guido, david]:\n",
    "    if msg := not_valid(user, schema):\n",
    "        print(msg, \"\\n--------------------\")\n",
    "    else:\n",
    "        print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "4271dea8a81ed122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guido van Rossum validates correctly\n",
      "User David Mertz validates correctly\n"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The schema in this first pass suffices to check the constraint the server in the prior lesson imposed.  In fact, it checks slightly more in guaranteeing that the field \"name\" is a string.\n",
   "id": "fd68edfc714b6c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:24:51.169198Z",
     "start_time": "2025-07-16T13:24:51.162285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "barbara_feldon = json.loads(\"\"\"{\n",
    "  \"name\": 99,\n",
    "  \"details\": {\"profession\": \"CONTROL Agent\"}\n",
    "}\"\"\")"
   ],
   "id": "48873ea3c52ee7c0",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have two not-quite-conformant user JSON documents to validate. Each fails in a different way.",
   "id": "22d1b06c27897845"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:25:19.706665Z",
     "start_time": "2025-07-16T13:25:19.699108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for user in [barbara_feldon, intruder]:\n",
    "    if msg := not_valid(user, schema):\n",
    "        print(msg, \"\\n--------------------\")\n",
    "    else:\n",
    "        print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "99223b9cc0870d1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 is not of type 'string'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['name']:\n",
      "    {'type': 'string'}\n",
      "\n",
      "On instance['name']:\n",
      "    99 \n",
      "--------------------\n",
      "'name' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'type': 'object',\n",
      "     'required': ['name'],\n",
      "     'properties': {'name': {'type': 'string'}}}\n",
      "\n",
      "On instance:\n",
      "    {'password': 'P4cC!^*8chWz8', 'profession': 'Hacker'} \n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Nested Structure\n",
    "\n",
    "A JSON Schema allows specification of nested structures, including type and cardinality, and also may optionally contain a number of annotations to describe the schema itself.  Let us add a few. In the expanded schema, we will require a password along with a name.  Notice that we describe several aspects of what the field \"lucky_numbers\" might look like, but we do not make it required.  Guido had none, but David did; both should validate."
   ],
   "id": "8aa6fb4ed9925d2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:25:42.307306Z",
     "start_time": "2025-07-16T13:25:42.299969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "schema = json.loads(\"\"\"{\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"$id\": \"http://example.com/user.schema.json\",\n",
    "  \"title\": \"User\",\n",
    "  \"description\": \"A User of Our Computer System\",\n",
    "  \"type\" : \"object\",\n",
    "  \"required\": [\"name\", \"password\"],\n",
    "  \"properties\" : {\n",
    "     \"name\" : {\"type\" : \"string\"},\n",
    "     \"password\": {\n",
    "         \"description\": \"Use special characters and mixed case\",\n",
    "         \"type\": \"string\"},\n",
    "     \"lucky_numbers\": {\n",
    "         \"description\": \"Up to 6 favorite numbers 1-100\",\n",
    "         \"type\": \"array\",\n",
    "         \"items\": {\n",
    "           \"type\": \"number\",\n",
    "           \"minimum\": 1,\n",
    "           \"maximum\": 100\n",
    "         },\n",
    "         \"uniqueItems\": true,\n",
    "         \"minItems\": 0,\n",
    "         \"maxItems\": 6\n",
    "    }\n",
    "  }\n",
    "}\"\"\")"
   ],
   "id": "70edecf161dd65fb",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:25:52.766691Z",
     "start_time": "2025-07-16T13:25:52.760959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for user in [guido, david]:\n",
    "    if msg := not_valid(user, schema):\n",
    "        print(msg, \"\\n--------------------\")\n",
    "    else:\n",
    "        print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "3826188552f2b67e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guido van Rossum validates correctly\n",
      "User David Mertz validates correctly\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are a few ways that validation might fail with the expanded schema.  Obviously, \"password\" was added as a required field, but the pattern there is identical as with \"name\".  The field \"lucky_numbers\" has more going on.  It might be omitted altogether for a valid users, but if it is included, it can only be an array (Python list) of numbers between 1 and 100; moreover, it can only have from zero to six numbers that must be distinct.",
   "id": "f27bb57d3c27c331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:26:23.960223Z",
     "start_time": "2025-07-16T13:26:23.948900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "the_count = json.loads(\"\"\"{\n",
    "  \"name\": \"Count von Count\",\n",
    "  \"password\": \"fourbananas\",\n",
    "  \"lucky_numbers\": [\"one\", \"two\", \"three\"]\n",
    "}\"\"\")\n",
    "\n",
    "if msg := not_valid(the_count, schema):\n",
    "    print(msg, \"\\n--------------------\")\n",
    "else:\n",
    "    print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "2cae1a2eaa564ffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'three' is not of type 'number'\n",
      "\n",
      "Failed validating 'type' in schema['properties']['lucky_numbers']['items']:\n",
      "    {'type': 'number', 'minimum': 1, 'maximum': 100}\n",
      "\n",
      "On instance['lucky_numbers'][2]:\n",
      "    'three' \n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:26:37.695903Z",
     "start_time": "2025-07-16T13:26:37.681835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cantor = json.loads(\"\"\"{\n",
    "  \"name\": \"Georg Cantor\",\n",
    "  \"password\": \"omega_aleph\",\n",
    "  \"lucky_numbers\": [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "}\"\"\")\n",
    "\n",
    "if msg := not_valid(cantor, schema):\n",
    "    print(msg, \"\\n--------------------\")\n",
    "else:\n",
    "    print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "49ad4808c4889be2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8] is too long\n",
      "\n",
      "Failed validating 'maxItems' in schema['properties']['lucky_numbers']:\n",
      "    {'description': 'Up to 6 favorite numbers 1-100',\n",
      "     'type': 'array',\n",
      "     'items': {'type': 'number', 'minimum': 1, 'maximum': 100},\n",
      "     'uniqueItems': True,\n",
      "     'minItems': 0,\n",
      "     'maxItems': 6}\n",
      "\n",
      "On instance['lucky_numbers']:\n",
      "    [1, 2, 3, 4, 5, 6, 7, 8] \n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:26:50.990437Z",
     "start_time": "2025-07-16T13:26:50.985622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "revolution_9 = json.loads(\"\"\"{\n",
    "  \"name\": \"Yoko Ono\",\n",
    "  \"password\": \"grapefruit\",\n",
    "  \"lucky_numbers\": [9, 9, 9]\n",
    "}\"\"\")\n",
    "\n",
    "if msg := not_valid(revolution_9, schema):\n",
    "    print(msg, \"\\n--------------------\")\n",
    "else:\n",
    "    print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "e9d6c70c6c12525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 9, 9] has non-unique elements\n",
      "\n",
      "Failed validating 'uniqueItems' in schema['properties']['lucky_numbers']:\n",
      "    {'description': 'Up to 6 favorite numbers 1-100',\n",
      "     'type': 'array',\n",
      "     'items': {'type': 'number', 'minimum': 1, 'maximum': 100},\n",
      "     'uniqueItems': True,\n",
      "     'minItems': 0,\n",
      "     'maxItems': 6}\n",
      "\n",
      "On instance['lucky_numbers']:\n",
      "    [9, 9, 9] \n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-16T13:27:10.149905Z",
     "start_time": "2025-07-16T13:27:10.136829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "go_big = json.loads(\"\"\"{\n",
    "  \"name\": \"Leslie Knope\",\n",
    "  \"password\": \"ilovepawnee\",\n",
    "  \"lucky_numbers\": [1000000, 200000]\n",
    "}\"\"\")\n",
    "\n",
    "if msg := not_valid(go_big, schema):\n",
    "    print(msg, \"\\n--------------------\")\n",
    "else:\n",
    "    print(f\"User {user['name']} validates correctly\")"
   ],
   "id": "fc5511b88a5b09d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 is greater than the maximum of 100\n",
      "\n",
      "Failed validating 'maximum' in schema['properties']['lucky_numbers']['items']:\n",
      "    {'type': 'number', 'minimum': 1, 'maximum': 100}\n",
      "\n",
      "On instance['lucky_numbers'][1]:\n",
      "    200000 \n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reading CSV with Standard Library\n",
    "\n",
    "Python provides a module in its standard library for reading and writing CSV or other delimited files.  It can be tempting to create or read such files using only Python's powerful string manipulation functionality.  Indeed, the author of this tutorial has done so far more often than he wishes to admit; however, it is a mistake to eschew the `csv` module which simply deals with many edge cases that are easy to overlook in quick scripts."
   ],
   "id": "116d7ee18480802e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us start out by loading a few Python standard library modules that this lesson will utilize.",
   "id": "b1cf3caebc54ee19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:38:37.503226Z",
     "start_time": "2025-07-17T07:38:37.491497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from pprint import pprint\n",
    "from collections import namedtuple\n",
    "from decimal import Decimal"
   ],
   "id": "22974a8110508a7c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Doing it Wrong\n",
    "\n",
    "In Python, the string methods `.split()` and `.join()` do 90% of what we need to in working with CSV.  The problem is, they do not do the other 10%.  Let's try a naive approach that goes bad."
   ],
   "id": "7bb37557bb061341"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:53:48.562108Z",
     "start_time": "2025-07-17T14:53:48.553631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fields = [\"Name\", \"Evaluation\", \"Rating\", \"Age\"]\n",
    "data = [\n",
    "    [\"Mia Johnson\", \"The movie was excellent\", 9.5, 25],\n",
    "    [\"Liam Lopez\", \"Didn't really like it\", 3.0, 35],\n",
    "    [\"Isabella Lee\", \"Wow! That was great\", 8.0, 45,]\n",
    "]"
   ],
   "id": "bdaeaa5b21e3594a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is unremarkable data about several movie evaluations.  Let us try to serialize it.",
   "id": "87cd8e2aa4efa705"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the files associated with this course",
   "id": "61dd80bda161c1a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:42:34.679806Z",
     "start_time": "2025-07-17T07:42:34.666265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv', 'w') as movie:\n",
    "    try:\n",
    "        print(\",\".join(fields), file=movie)\n",
    "        for record in data:\n",
    "            print(\",\".join(record), file=movie)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ],
   "id": "ebfdc2b9dbe97292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence item 2: expected str instance, float found\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is easy to see what went wrong.  The `.join()` method needs only strings in the iterable argument.  We can fix that fairly easily.  Python knows how to *stringify* all its objects.",
   "id": "296bde48a1e250f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:45:07.155517Z",
     "start_time": "2025-07-17T07:45:07.146587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv', 'w') as movie:\n",
    "    try:\n",
    "        print(\",\".join(fields), file=movie)\n",
    "        for record in data:\n",
    "            print(\",\".join(str(r) for r in record), file=movie)\n",
    "    except Exception as err:\n",
    "        print(err)"
   ],
   "id": "893a3ae620cc528f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Success! At least for now. Perhaps we want to read it back as a list of dictionaries.",
   "id": "dd8da26c8c60babe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We need to read the header first to use as keys, then we can pull values from each corresponding position in later rows.",
   "id": "167f363f1f4eca6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:46:12.781388Z",
     "start_time": "2025-07-17T07:46:12.770349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as movie:\n",
    "    newdata = []\n",
    "    keys = next(movie).split(',') # Header\n",
    "    for line in movie:\n",
    "        newdata.append(dict(zip(keys, line.split(','))))\n",
    "\n",
    "pprint(newdata)"
   ],
   "id": "9d098f9201410267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Age\\n': '25\\n',\n",
      "  'Evaluation': 'The movie was excellent',\n",
      "  'Name': 'Mia Johnson',\n",
      "  'Rating': '9.5'},\n",
      " {'Age\\n': '35\\n',\n",
      "  'Evaluation': \"Didn't really like it\",\n",
      "  'Name': 'Liam Lopez',\n",
      "  'Rating': '3.0'},\n",
      " {'Age\\n': '45\\n',\n",
      "  'Evaluation': 'Wow! That was great',\n",
      "  'Name': 'Isabella Lee',\n",
      "  'Rating': '8.0'}]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We did *pretty well*.  However, the last field of header and data have a trailing newline chacter we do not really want.  We can strip that, but other problems still arise.",
   "id": "7cc30c1fa3eaada9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:46:46.911311Z",
     "start_time": "2025-07-17T07:46:46.905098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as movie:\n",
    "    newdata = []\n",
    "    line = next(movie).rstrip()  # Header\n",
    "    keys = line.split(',')\n",
    "    for line in movie:\n",
    "        line = line.rstrip()\n",
    "        newdata.append(dict(zip(keys, line.split(','))))\n",
    "\n",
    "pprint(newdata)"
   ],
   "id": "f09a4f6da6bd0f8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Age': '25',\n",
      "  'Evaluation': 'The movie was excellent',\n",
      "  'Name': 'Mia Johnson',\n",
      "  'Rating': '9.5'},\n",
      " {'Age': '35',\n",
      "  'Evaluation': \"Didn't really like it\",\n",
      "  'Name': 'Liam Lopez',\n",
      "  'Rating': '3.0'},\n",
      " {'Age': '45',\n",
      "  'Evaluation': 'Wow! That was great',\n",
      "  'Name': 'Isabella Lee',\n",
      "  'Rating': '8.0'}]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that something is going to go wrong when a field can legitimately contain the delimiter.",
   "id": "f5f84c98470cf499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:48:51.074659Z",
     "start_time": "2025-07-17T07:48:50.926686Z"
    }
   },
   "cell_type": "code",
   "source": "!cat data/movie.csv",
   "id": "2eafb2444b6b3f50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Evaluation,Rating,Age\r\n",
      "Mia Johnson,The movie was excellent,9.5,25\r\n",
      "Liam Lopez,Didn't really like it,3.0,35\r\n",
      "Isabella Lee,Wow! That was great,8.0,45\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's use the idential ad hoc reader to read the data on disk again.",
   "id": "3fe8018b217ca1c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:48:17.697149Z",
     "start_time": "2025-07-17T07:48:17.682543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as movie:\n",
    "    newdata = []\n",
    "    line = next(movie).rstrip()\n",
    "    keys = line.split(',') # Header\n",
    "    for line in movie:\n",
    "        line = line.rstrip()\n",
    "        newdata.append(dict(zip(keys, line.split(','))))\n",
    "\n",
    "pprint(newdata)"
   ],
   "id": "f31332b9edced1e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Age': '25',\n",
      "  'Evaluation': 'The movie was excellent',\n",
      "  'Name': 'Mia Johnson',\n",
      "  'Rating': '9.5'},\n",
      " {'Age': '35',\n",
      "  'Evaluation': \"Didn't really like it\",\n",
      "  'Name': 'Liam Lopez',\n",
      "  'Rating': '3.0'},\n",
      " {'Age': '45',\n",
      "  'Evaluation': 'Wow! That was great',\n",
      "  'Name': 'Isabella Lee',\n",
      "  'Rating': '8.0'}]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As written, nothing crashed.  But we also get data in the wrong fields sometimes. Another likely problem is handling embedded newlines in strings; a few other edge cases also occur. We could complicate matters with some additional code, and eventually get it right.  But the Python standard library does that for us.",
   "id": "d4e567ee418c6e9b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The `csv` Module\n",
    "\n",
    "In the basic case, using the `csv` module gives us a largely file-like interface.  It merely handles a few things that can go wrong automatically."
   ],
   "id": "9ba48472d711360d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:51:38.009567Z",
     "start_time": "2025-07-17T07:51:37.853698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv', 'w') as fh:\n",
    "    movie = csv.writer(fh, quoting=csv.QUOTE_MINIMAL)\n",
    "    for record in [fields]+data:\n",
    "        movie.writerow(record)\n",
    "\n",
    "!cat data/movie.csv"
   ],
   "id": "e14550ac47e5619e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Evaluation,Rating,Age\r\n",
      "Mia Johnson,The movie was excellent,9.5,25\r\n",
      "Liam Lopez,Didn't really like it,3.0,35\r\n",
      "Isabella Lee,Wow! That was great,8.0,45\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reading the data back is similar, with quoting and escaping handled properly.",
   "id": "49c6fcb36a936622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:52:05.072617Z",
     "start_time": "2025-07-17T07:52:05.063805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as fh:\n",
    "    movie = csv.reader(fh)\n",
    "    for record in movie:\n",
    "        print(record)"
   ],
   "id": "1f5645065f9929c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Evaluation', 'Rating', 'Age']\n",
      "['Mia Johnson', 'The movie was excellent', '9.5', '25']\n",
      "['Liam Lopez', \"Didn't really like it\", '3.0', '35']\n",
      "['Isabella Lee', 'Wow! That was great', '8.0', '45']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Typing\n",
    "\n",
    "Unlike some other tools, the standard library `csv` module makes little attempt to impose datatypes.  During writing, it will, of course, stringify objects that are not strings.  It usually leaves the decision of casting to other types up to the programmer."
   ],
   "id": "fac0cafd62367857"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:53:08.044296Z",
     "start_time": "2025-07-17T07:53:07.901798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv', 'w') as fh:\n",
    "    movie = csv.writer(fh, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for record in [fields]+data:\n",
    "        movie.writerow(record)\n",
    "\n",
    "!cat data/movie.csv"
   ],
   "id": "db9fe5b25092b01a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Name\",\"Evaluation\",\"Rating\",\"Age\"\r\n",
      "\"Mia Johnson\",\"The movie was excellent\",9.5,25\r\n",
      "\"Liam Lopez\",\"Didn't really like it\",3.0,35\r\n",
      "\"Isabella Lee\",\"Wow! That was great\",8.0,45\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The `csv` module provides a limited option to quote all strings and to infer that anything unquoted is a number instead.  The numeric type used is always a floating point for this rule.  If you wish to read in an int, or a Decimal or Fraction, or another numeric type, you still need to write more custom code.",
   "id": "56581f5f8ddac5ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:53:48.263742Z",
     "start_time": "2025-07-17T07:53:48.255515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as fh:\n",
    "    movie = csv.reader(fh, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for record in movie:\n",
    "        print(record)"
   ],
   "id": "89796ac0f7cd1cf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Evaluation', 'Rating', 'Age']\n",
      "['Mia Johnson', 'The movie was excellent', 9.5, 25.0]\n",
      "['Liam Lopez', \"Didn't really like it\", 3.0, 35.0]\n",
      "['Isabella Lee', 'Wow! That was great', 8.0, 45.0]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Probably what we really want is to specify various data types for various columns.  In the example, Age is probably meant as in integer and Rating as a fractional number.  While we are customizing, perhaps a different collection type than a list is a more descriptive way to store records.",
   "id": "6d8fed418fb840e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:54:19.084661Z",
     "start_time": "2025-07-17T07:54:19.068064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify special types, string by default\n",
    "types = {'Age': int, 'Rating': Decimal}\n",
    "\n",
    "with open('data/movie.csv') as fh:\n",
    "    newdata = []\n",
    "    # Create a descriptive record for this data\n",
    "    movie = csv.reader(fh)\n",
    "    fields = next(movie)\n",
    "    Movie = namedtuple(\"Movie\", fields)\n",
    "    for record in movie:\n",
    "        # Cast each item to its needed datatype\n",
    "        for pos, datum in enumerate(record):\n",
    "            cast = types.get(fields[pos], str)\n",
    "            record[pos] = cast(datum)\n",
    "        newdata.append(Movie(*record))\n",
    "\n",
    "pprint(newdata)"
   ],
   "id": "384fcf05b9af451d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Movie(Name='Mia Johnson', Evaluation='The movie was excellent', Rating=Decimal('9.5'), Age=25),\n",
      " Movie(Name='Liam Lopez', Evaluation=\"Didn't really like it\", Rating=Decimal('3.0'), Age=35),\n",
      " Movie(Name='Isabella Lee', Evaluation='Wow! That was great', Rating=Decimal('8.0'), Age=45)]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Records as Dictionaries\n",
    "\n",
    "A namedtuple or a dataclass are useful Python standard library types for structured collections. The built-in standard type for doing that is a dictionary.  The `csv` module includes classes to make reading or writing dicts convenient.  These different types have pros and cons, but all are useful.  Dictionaries come with convenience classes to make similar code slightly shorter."
   ],
   "id": "d325b8a217cf20b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:54:35.568069Z",
     "start_time": "2025-07-17T07:54:35.561107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.csv') as fh:\n",
    "    movie = csv.DictReader(fh, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for record in movie:\n",
    "        print(record)"
   ],
   "id": "69c7c07544773aa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Mia Johnson', 'Evaluation': 'The movie was excellent', 'Rating': 9.5, 'Age': 25.0}\n",
      "{'Name': 'Liam Lopez', 'Evaluation': \"Didn't really like it\", 'Rating': 3.0, 'Age': 35.0}\n",
      "{'Name': 'Isabella Lee', 'Evaluation': 'Wow! That was great', 'Rating': 8.0, 'Age': 45.0}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Writing dictionaries back out to CSV is very similar.  Here we add a minor option, `newline=` in order to be able to write records with newlines in strings.  While we are doing that, let us also use a different delimiter to demonstrate that.",
   "id": "9f02e56fadd3dea2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:54:55.152416Z",
     "start_time": "2025-07-17T07:54:55.004334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.txt', 'w', newline='') as fh:\n",
    "    fields = ['Name', 'Rating', 'Age']\n",
    "    movie = csv.DictWriter(fh, fieldnames=fields, delimiter=\"|\")\n",
    "    movie.writeheader()\n",
    "    movie.writerow({'Name': 'Mia\\nJohnson', 'Rating': 9.5, 'Age': 25})\n",
    "    movie.writerow({'Age': 35, 'Name': 'Liam Lopez'})\n",
    "    movie.writerow({'Name': 'Isabella \"Bella\" Lee', 'Rating': 8.0, 'Age': 45})\n",
    "\n",
    "!cat data/movie.txt"
   ],
   "id": "90f1ac531b12dba6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name|Rating|Age\r\n",
      "\"Mia\r\n",
      "Johnson\"|9.5|25\r\n",
      "Liam Lopez||35\r\n",
      "\"Isabella \"\"Bella\"\" Lee\"|8.0|45\r\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Despite the slightly surprising newline inside a field, this will round-trip perfectly fine because of the quote.  The quotes inside one of the fields are also handle correctly.",
   "id": "7a18d0d50b344ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T07:55:25.712778Z",
     "start_time": "2025-07-17T07:55:25.704089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/movie.txt', newline='') as fh:\n",
    "    movie = csv.DictReader(fh, delimiter=\"|\")\n",
    "    for record in movie:\n",
    "        print(record)"
   ],
   "id": "e386911707782d5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Mia\\nJohnson', 'Rating': '9.5', 'Age': '25'}\n",
      "{'Name': 'Liam Lopez', 'Rating': '', 'Age': '35'}\n",
      "{'Name': 'Isabella \"Bella\" Lee', 'Rating': '8.0', 'Age': '45'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reading CSV with Pandas\n",
    "\n",
    "If it is available in your environment, the `Pandas` package provides a versatile, flexible, and fast reader and writer of CSV and other delimited files.  Moreover, when read, delimited files are read into a flexible data structure called a DataFrame that has numerous useful methods.  The Pandas library can perform a great deal of work for data processing and data manipulation, but most of that is outside the scope of this lesson."
   ],
   "id": "a95ae50218c15a5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us start out by loading the Pandas library and the CSV module.  Pandas is conventionally loaded as the short name `pd`.  Similarly, `NumPy` is conventionally loaded as `np`.",
   "id": "6c8dc18989c41688"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:54:19.699123Z",
     "start_time": "2025-07-17T14:54:19.690278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ],
   "id": "25d964ff85d72bcb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Basic Reading\n",
    "\n",
    "In principle, Pandas provides a huge number of options for reading CSV or other delimited files.  In fact, it has readers for a huge number of entirely different data formats as well.  In the simple case, it could hardly be simpler.  Let us look at a CSV file then read it to a DataFrame."
   ],
   "id": "51bcbd9ee9107f36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the files associated with this course",
   "id": "74a8274b715b08a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T13:53:58.100922Z",
     "start_time": "2025-07-17T13:53:57.970154Z"
    }
   },
   "cell_type": "code",
   "source": "!cat data/movie.csv",
   "id": "b2de7cdc3c8585c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Name\",\"Evaluation\",\"Rating\",\"Age\"\r\n",
      "\"Mia Johnson\",\"The movie was excellent\",9.5,25\r\n",
      "\"Liam Lopez\",\"Didn't really like it\",3.0,35\r\n",
      "\"Isabella Lee\",\"Wow! That was great\",8.0,45\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T13:55:11.967268Z",
     "start_time": "2025-07-17T13:55:11.912448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/movie.csv')\n",
    "df"
   ],
   "id": "7050a965038bdad2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Name               Evaluation  Rating  Age\n",
       "0   Mia Johnson  The movie was excellent     9.5   25\n",
       "1    Liam Lopez    Didn't really like it     3.0   35\n",
       "2  Isabella Lee      Wow! That was great     8.0   45"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mia Johnson</td>\n",
       "      <td>The movie was excellent</td>\n",
       "      <td>9.5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liam Lopez</td>\n",
       "      <td>Didn't really like it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Isabella Lee</td>\n",
       "      <td>Wow! That was great</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Types\n",
    "Or interest here especially is the type inference that was performed by Pandas. Thins that look like integers get converted to integers, things that look like floats get converted to floats."
   ],
   "id": "a67da01ce1f94730"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:01:44.032088Z",
     "start_time": "2025-07-17T14:01:44.007160Z"
    }
   },
   "cell_type": "code",
   "source": "df.dtypes",
   "id": "f73ec136ebedfb9d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           object\n",
       "Evaluation     object\n",
       "Rating        float64\n",
       "Age             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Explicit Typing\n",
    "Panda lets you specify the type of the columns explicitly, inasmuch as datatypes make sense for a given column. For the most part, this is useful only to encode in fewer bits or to explicitly use floats where a column might be interred as integer."
   ],
   "id": "d40cdb07bc8f10fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:25:38.201397Z",
     "start_time": "2025-07-17T14:25:38.159129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\n",
    "    'data/movie.csv',\n",
    "    dtype={'Age': np.float16, 'Rating': np.float64}\n",
    ")\n",
    "df.dtypes"
   ],
   "id": "a030b0993845fe82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name           object\n",
       "Evaluation     object\n",
       "Rating        float64\n",
       "Age           float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parsing Dates\n",
    "pandas goes further than the standard library `csv` module can in also optionally parsing dates.The next example not only has dates in an extra field, but in fact encodes the dates in multiple different formats.If parsing a column as a data is specified,Pandas will attempt a large collection of heuristic rule to guess at what formate was intended.One of the dates given is accompanied by a particular time as well, down to a fraction of a second."
   ],
   "id": "ceb54271b3dda77d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the file associated with this course",
   "id": "f20903448e037705"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:56:10.110871Z",
     "start_time": "2025-07-17T14:56:09.980075Z"
    }
   },
   "cell_type": "code",
   "source": "!cat data/movie.csv",
   "id": "c2d2effb426b21ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Name\",\"Evaluation\",\"Rating\",\"Age\"\r\n",
      "\"Mia Johnson\",\"The movie was excellent\",9.5,25\r\n",
      "\"Liam Lopez\",\"Didn't really like it\",3.0,35\r\n",
      "\"Isabella Lee\",\"Wow! That was great\",8.0,45\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:58:47.415717Z",
     "start_time": "2025-07-17T14:58:47.287282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"Date\"] = datetime.today().strftime('%Y-%m-%d')\n",
    "df.to_csv('data/movie.csv', index=False)\n",
    "!cat data/movie.csv"
   ],
   "id": "5ff71b2b872bca18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Evaluation,Rating,Age,date,Date\r\n",
      "Mia Johnson,The movie was excellent,9.5,25.0,2025-07-17,2025-07-17\r\n",
      "Liam Lopez,Didn't really like it,3.0,35.0,2025-07-17,2025-07-17\r\n",
      "Isabella Lee,Wow! That was great,8.0,45.0,2025-07-17,2025-07-17\r\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T14:58:48.829353Z",
     "start_time": "2025-07-17T14:58:48.820575Z"
    }
   },
   "cell_type": "code",
   "source": "pd.read_csv('data/movie.csv', parse_dates=['date'])",
   "id": "16ab786f9848583b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Name               Evaluation  Rating   Age       date        Date\n",
       "0   Mia Johnson  The movie was excellent     9.5  25.0 2025-07-17  2025-07-17\n",
       "1    Liam Lopez    Didn't really like it     3.0  35.0 2025-07-17  2025-07-17\n",
       "2  Isabella Lee      Wow! That was great     8.0  45.0 2025-07-17  2025-07-17"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Age</th>\n",
       "      <th>date</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mia Johnson</td>\n",
       "      <td>The movie was excellent</td>\n",
       "      <td>9.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>2025-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Liam Lopez</td>\n",
       "      <td>Didn't really like it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>2025-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Isabella Lee</td>\n",
       "      <td>Wow! That was great</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>2025-07-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Format Variations\n",
    "\n",
    "By choosing from various available parameters, the same `pd.read_csv()` function can read most delimited formats.  For example, we can read the pipe (`|`) delimited file created in the last lesson that also had embedded newlines and quotes. The special value `NaN` (Not a Number) is used to mark missing data."
   ],
   "id": "c2eb9f06077d6fc1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the files associated with this course",
   "id": "81b96ef63016360"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T15:29:37.693910Z",
     "start_time": "2025-07-17T15:29:37.557293Z"
    }
   },
   "cell_type": "code",
   "source": "!cat data/movie.txt",
   "id": "ab2cb9ac03e83c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name|Rating|Age\r\n",
      "\"Mia\r\n",
      "Johnson\"|9.5|25\r\n",
      "Liam Lopez||35\r\n",
      "\"Isabella \"\"Bella\"\" Lee\"|8.0|45\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some of the parameters used in the below example are simply their default values. They are shown to illustrate the range of options.",
   "id": "855eef5715e3a256"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('data/movie.txt', \n",
    "                 sep=\"|\", \n",
    "                 nrows=100, \n",
    "                 skip_blank_lines=True, \n",
    "                 decimal='.', \n",
    "                 quotechar='\"')\n",
    "df"
   ],
   "id": "79db6a18e7a27d97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sometimes you will encounter CSV or other delimited files without headers.  A few options can handle that.  If we do not give parameters to indicate this, the DataFrame will be confused.",
   "id": "5eb98e472e42e35c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> The data associated with this notebook can be found in the files associated with this course",
   "id": "aab3d19a3ef005f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.read_csv('data/movie-noheader.csv')",
   "id": "aedd18b3876d981c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pd.read_csv('data/movie-noheader.csv', \n",
    "             names=['Person', 'Description', 'Score', 'Age'])"
   ],
   "id": "ed4b01142d3bc0d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exporting to CSV\n",
    "\n",
    "Once you *have* a Pandas DataFrame, whether constructed from scratch, read from any of numerous data formats, modified and filtered using Pandas methods, or whatever, it is easy to export it to a new CSV file.  This is not as completely general purpose as the Python `csv` module in that it is only a DataFrame that can do the writing, not arbitrary arrangements of data that you have manually programmed to write as records.  However, it is extremely straightforward, and allows generally the same numerous parameters as the reader."
   ],
   "id": "2f3dd76f827c55cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Notice automatic compression based on extension\n",
    "df.to_csv('data/movie.tsv.gz', \n",
    "          sep='\\t', \n",
    "          na_rep=\"N/A\",\n",
    "          quotechar=\"'\")"
   ],
   "id": "ef9997a2718f426b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!zcat data/movie.tsv.gz",
   "id": "32f4b3cf84eb550d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
